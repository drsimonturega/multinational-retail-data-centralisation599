{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Project Code test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get store from web API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5742/700821359.py:149: FutureWarning: Passing literal json to 'read_json' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  self.df_dates = pd.read_json(self.df_dates)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  timestamp month  year day time_period                             date_uuid\n",
      "0  22:00:06     9  2012  19     Evening  3b7ca996-37f9-433f-b6d0-ce8391b615ad\n",
      "1  22:44:06     2  1997  10     Evening  adc86836-6c35-49ca-bb0d-65b6507a00fa\n",
      "2  10:05:37     4  1994  15     Morning  5ff791bf-d8e0-4f86-8ceb-c7b60bef9b31\n",
      "3  17:29:27    11  2001   6      Midday  1b01fcef-5ab9-404c-b0d4-1e75a0bd19d8\n",
      "4  22:40:33    12  2015  31     Evening  dfa907c1-f6c5-40f0-aa0d-40ed77ac5a44\n",
      "percentage of missing values in each column:\n",
      "timestamp      0.0\n",
      "month          0.0\n",
      "year           0.0\n",
      "day            0.0\n",
      "time_period    0.0\n",
      "date_uuid      0.0\n",
      "dtype: float64\n",
      "percentage of missing values in each column:\n",
      "timestamp      0.0\n",
      "month          0.0\n",
      "year           0.0\n",
      "day            0.0\n",
      "time_period    0.0\n",
      "date_uuid      0.0\n",
      "dtype: float64\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tabula\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "class DataExtractor:\n",
    "\n",
    "\n",
    "# class constructor\n",
    "    def __init__(self):\n",
    "        self.run_test()\n",
    "        return\n",
    "    \n",
    "\n",
    "    def list_number_of_stores(self):\n",
    "        url = 'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/number_stores'\n",
    "        headers = {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "        self.n_stores = requests.get(url, headers=headers)\n",
    "        self.n_stores = self.n_stores.json()\n",
    "        self.n_stores = self.n_stores['number_stores'] \n",
    "        return self.n_stores\n",
    "    \n",
    "    def retrieve_stores_data(self):\n",
    "        headers = {'x-api-key': 'yFBQbwXe9J3sd6zWVAMrK6lcxxr0q1lr2PT6DDMX'}\n",
    "        self.df_stores = pd.DataFrame()\n",
    "        for n in range(1, self.n_stores):\n",
    "            url = (f'https://aqj7u5id95.execute-api.eu-west-1.amazonaws.com/prod/store_details/{n}')\n",
    "            self.stores_trans = requests.get(url, headers=headers)\n",
    "            self.stores_trans = self.stores_trans.json()\n",
    "            col_lst = self.stores_trans.keys()\n",
    "            val_lst = self.stores_trans.values()\n",
    "            self.df_trans = pd.DataFrame([val_lst])\n",
    "            self.df_trans.columns = col_lst\n",
    "            self.df_stores = pd.concat([self.df_stores, self.df_trans])\n",
    "        self.df_stores.set_index('index', inplace = True)\n",
    "        #print(self.df_stores.head())\n",
    "        return self.df_stores\n",
    "\n",
    "    def clean_store_data(self):\n",
    "        #print(self.df_stores.head())\n",
    "        print(\"percentage of missing values in each column:\")\n",
    "        print(self.df_stores.isna().mean() * 100)\n",
    "        self.df_stores = self.df_stores.dropna(axis = 1)\n",
    "        inv_cols = ['opening_date']\n",
    "        for c_cols in inv_cols:\n",
    "            self.df_stores[c_cols] = pd.to_datetime(self.df_stores[c_cols], errors='coerce')\n",
    "        self.df_stores = self.df_stores.dropna(axis = 0)\n",
    "        print(\"percentage of missing values in each column:\")\n",
    "        print(self.df_stores.isna().mean() * 100)\n",
    "        n_unique = self.df_stores['store_code'].nunique()\n",
    "        print(f'Total records {len(self.df_stores)}, unique card numbers {n_unique}')\n",
    "        #print(self.df_stores.head())\n",
    "        return\n",
    "\n",
    "    def extract_from_s3(self):\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.get_object(Bucket='data-handling-public', Key='products.csv')\n",
    "        self.df_prod = pd.read_csv(response.get(\"Body\"))\n",
    "        col_lst = self.df_prod.columns.values.tolist()\n",
    "        col_lst[0] = 'index'\n",
    "        self.df_prod.columns = col_lst\n",
    "        self.df_prod.set_index('index', inplace = True)\n",
    "        #print(self.df_prod.head())\n",
    "        return self.df_prod\n",
    "    \n",
    "    def convert_product_weights_II(self):\n",
    "        # this may need work\n",
    "        temp = pd.DataFrame()\n",
    "        temp['mul_no'] = []\n",
    "        temp['mul_weight'] = []\n",
    "        temp['units'] = self.df_prod['weight'].str.extract(r'(\"|g|kg|ml\")')\n",
    "        temp['weight'] = self.df_prod['weight'].str.replace(r'(\"|g|kg|ml\")', '', regex=True)\n",
    "        temp['weight'] = temp['weight'].astype(str)\n",
    "        temp.insert(4,'multi',temp['units'])\n",
    "        for n in range(0,len(temp)):\n",
    "            if ' x ' in temp.iloc[n,3]:\n",
    "               temp.iloc[n,0], temp.iloc[n,1] = temp.iloc[n,3].split(' x ')\n",
    "            if ' .' in temp.iloc[n,3]:\n",
    "                temp.iloc[n,3] = temp.iloc[n,3].replace(\" .\", \"\")\n",
    "        temp['mul_no'] = temp['mul_no'].astype(float)\n",
    "        temp['mul_weight'] = temp['mul_weight'].astype(float)\n",
    "        for n in range(0,len(temp)):\n",
    "            if 'x' in temp.iloc[n,3]:\n",
    "                temp.iloc[n,3] = temp.iloc[n,0] * temp.iloc[n,1]\n",
    "            if temp.iloc[n,2] == 'NaN':\n",
    "                temp.iloc[n,3] = None\n",
    "            if  temp.iloc[n,2] == 'g':\n",
    "               temp.iloc[n,3] = float(temp.iloc[n,3]) / 1000\n",
    "               temp.iloc[n,2] = 'kg'\n",
    "        self.df_prod['weight'] = temp['weight']  \n",
    "        return self.df_prod\n",
    "\n",
    "    \n",
    "    def convert_product_weights(self):\n",
    "        print(self.df_prod.iloc[3,2])\n",
    "        #self.df_prod.iloc['weight'] = float(self.df_prod.iloc['weight'])\n",
    "        def unit_upd(sample):       \n",
    "            if 'kg' in sample:\n",
    "                sample = sample.replace(sample[-2:],'')\n",
    "            elif 'g' in sample:\n",
    "                sample = float(sample.replace(sample[-1:],'')) / 1000\n",
    "            elif 'ml' in sample:\n",
    "                sample = float(sample.replace(sample[-2:],'')) / 1000\n",
    "            elif 'l' in sample:\n",
    "                sample = sample.replace(sample[-1:],'')\n",
    "            return  sample\n",
    "        for n in range(0,len(self.df_prod)+1):\n",
    "            self.df_prod.iloc[n,2] = unit_upd(self.df_prod.iloc[n,2])\n",
    "        #print(self.df_prod.head())\n",
    "        return\n",
    "        \n",
    "    def clean_products_data(self):\n",
    "        print(\"percentage of missing values in each column:\")\n",
    "        print(self.df_prod.isna().mean() * 100)\n",
    "        self.dim_products = self.df_prod.dropna(how = 'any', axis = 0)\n",
    "        print(\"percentage of missing values in each column:\")\n",
    "        print(self.df_prod.isna().mean() * 100)\n",
    "        print(self.df_prod.head(200))\n",
    "        return\n",
    "    \n",
    "    def list_db_tables(self):  # can add external arguments\n",
    "        inspector = inspect(self.engine)\n",
    "        self.tab_lst = inspector.get_table_names()\n",
    "        #print(f'here is {self.tab_lst}')\n",
    "        return self.tab_lst\n",
    "\n",
    "    def read_rds_table(self, tab_name):  # can add external arguments\n",
    "        #cur_con = DatabaseConnector('db_creds.yaml')\n",
    "        #self.tab_lst = cur_con.tab_lst\n",
    "        print('check')\n",
    "        for tab in self.tab_lst:\n",
    "            name = (f'df_{tab}')\n",
    "            if 'users' in name:\n",
    "                self.df_users = pd.read_sql_table(tab, self.engine)\n",
    "                break\n",
    "        #print(self.df_users.head())\n",
    "        return  self.df_users\n",
    "\n",
    "    \n",
    "    def extract_dates_s3(self):\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.get_object(Bucket='data-handling-public', Key='date_details.json')\n",
    "        self.df_dates = response[\"Body\"].read().decode()\n",
    "        self.df_dates = pd.read_json(self.df_dates)\n",
    "        print(self.df_dates.head())\n",
    "        return self.df_dates\n",
    "\n",
    "    def clean_dates_data(self):\n",
    "        # errors with dates\n",
    "        #self.df_users = self.df_users.set_index('index', inplace = True)\n",
    "        #print(self.df_users.head())\n",
    "        print(\"percentage of missing values in each column:\")\n",
    "        print(self.df_dates.isna().mean() * 100)\n",
    "        self.df_clean = self.df_dates\n",
    "        print(\"percentage of missing values in each column:\")\n",
    "        print(self.df_clean.isna().mean() * 100)                 \n",
    "        return self.df_clean\n",
    "\n",
    "    def run_test(self):\n",
    "        self.extract_dates_s3()\n",
    "        self.clean_dates_data()\n",
    "        #self.clean_store_data()\n",
    "        #self.extract_from_s3()\n",
    "        #self.convert_product_weights_II()\n",
    "        #self.clean_products_data()\n",
    "        #self.list_db_tables()\n",
    "        print(f'End')\n",
    "        return\n",
    "\n",
    "test = DataExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_store_data(self):\n",
    "    print(self.df_stores.head())\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example: Read JSON from S3\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# For show, we handle a nested JSON file that we can limit with the JsonPath parameter\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# For show, we also handle a JSON where a single entry spans multiple lines\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Consider whether optimizePerformance is right for your workflow.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkContext\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mawsglue\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GlueContext\n\u001b[1;32m      9\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Example: Read JSON from S3\n",
    "# For show, we handle a nested JSON file that we can limit with the JsonPath parameter\n",
    "# For show, we also handle a JSON where a single entry spans multiple lines\n",
    "# Consider whether optimizePerformance is right for your workflow.\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "\n",
    "dynamicFrame = glueContext.create_dynamic_frame.from_options(\n",
    "    connection_type=\"s3\",\n",
    "    connection_options={\"paths\": [\"s3://s3path\"]},\n",
    "    format=\"json\",\n",
    "    format_options={\n",
    "        \"jsonPath\": \"$.id\",\n",
    "        \"multiline\": True,\n",
    "        # \"optimizePerformance\": True, -> not compatible with jsonPath, multiline\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "db_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
